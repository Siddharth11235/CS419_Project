 \documentclass[15pt]{article}
\usepackage{url}
\usepackage{setspace}               
\usepackage[superscript]{cite}      
\usepackage{graphicx}               
\usepackage[normalem]{ulem}   		
\graphicspath{ {Figures/} }         
\usepackage{caption} 
\usepackage{cite}
\usepackage{indentfirst} 
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsmath}  				
\textwidth=6.5in                    
\oddsidemargin=0.0in                
\usepackage{listings}
\usepackage{listings}
\usepackage{fancyhdr} 
\usepackage{longtable}
\usepackage[table]{xcolor}
\usepackage{hyperref}

\usepackage[
  separate-uncertainty = true,
  multi-part-units = repeat
]{siunitx}
\pagestyle{fancy}
\fancyhf{}
\lhead{Cancer Survival and Prognosis}
\rhead{Page \thepage}



\usepackage{color}   
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    citecolor=black,
    linktoc=all, 
    linkcolor=black,
}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 

\begin{document}
\begin{center}
\textsc{\LARGE CS419: Introduction to Machine Learning}\\[1.0cm]
\textsc{\Large Project Report}

\HRule \\[0.4cm]
{ \huge \bfseries LSTM Wordsworth}\\[0.15cm] 
\HRule \\[1.5cm]
\end{center}

\begin{minipage}{0.5\textwidth}
\begin{flushleft} \large
Siddharth Agarwal P17109 
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Prof. Preethi Jyothi \\
Computer Science and Engineering\\
IIT Bombay

\end{flushright}
\end{minipage}\\[2cm]

\begin{center}
\includegraphics[width=60mm]{IIT_Bombay_logo.png}
\end{center}

\bigskip

\newpage
\tableofcontents
\newpage

\section{Motivation}
A lot of work has been done over the past few decades in the domain of text generation. The primary targets for this domain have been based in prose for obvious reasons.  However, poetry is a domain of literature that deserves the reverence it receives. Poetry is a way to understand how language and symbol systems work. It is a worthy expression of emotion, or deep feelings, and aesthetics. Therefore it felt appropriate to attempt to generate poetry based on the work of possibly the most well-known poet of the old Romantic generation, William Wordsworth.

\section{Background}
Ever since Mikolov et al. demonstrated how to generate text using RNNs, neural language modelling has received a fever pitch of attention. Neural text generation has been thoroughly explored, with teams even coming up with algorithms that can amp up the emotional aspect of poetry (Misztal et al.).  One of the most interesting contributions here came in the form of a blog post by Andrej Karpathy, who demonstrated using RNNs to generate Shakespearean poetry. Multiple generative approaches have also seen success in the domain.

\section{Objective}
The initial objective here was to compare multiple discriminative and generative approaches to generate poetry and compare them via quantitative metrics such as perplexity and accuracy, and qualitatively, by asking people with academic experience in Literature about whether they would believe that the snippets presented to them were really from Wordsworth.

However, I couldn't finish the generative approaches in time for the report. So for this project, I have attempted an analysis that compares the currently trendy recurrent neural networks architectures.


\section{Dataset}
The dataset here is the first volume of Shakespearean poetry out of an eight volume collection available on Project Gutenberg.  
\subsection{Attributes of the Dataset}
\begin{center}
\begin{tabular}{c c}
\textbf{Attribute} & \textbf{Values} \\
No. of lines & 10902 \\
No. of word tokens & 91093 \\
Size of Vocabulary & 9940 \\
No. of Individual Characters & 46
\end{tabular}
\end{center}


%%% Algorithms 

\section{Algorithms}
\subsection{Preparing the dataset}
\subsubsection{Word Level}
\begin{itemize}
\item The data was stripped of any punctuation that was not era-appropriate.
\item The data was then tokenised with spaces as the separator.
\item The data was saved into a series of sequences each of lenght 11 tokens.
\item The vocabulary was then created as an unsorted set of the word tokens.
\end{itemize}
\subsubsection{Character Level}
\begin{itemize}
\item The data was stripped of any punctuation that was not era-appropriate.
\item The data was saved into a series of sequences each of lenght 50 tokens.
\item The vocabulary was then created as an unsorted set of the character indices as used in the standard Keras example.
\end{itemize}


\subsection*{Baseline Models}
The baseline models used were the standard Keras example on text generation and Andrej Karpathy's char-RNN. Both models were meant for larger datasets and therefore ended up overfitting very quickly. However, a  bigram maximum likelihood (counting) baseine [4] did end up giving interesting results.


\subsection{Character Level LSTM}
\begin{center}
\begin{tabular}{c c}
Number of LSTM layers & 1 \\
Number of dense layers & 1 \\
Number of nodes in the LSTM layer  & 135 \\
Categorical Crossentropy loss value & 1.4259 \\
\end{tabular}
\end{center}
The model, while displaying a very high loss value as compared to the word-level models performed well qualitatively. A sample poem has been attached below:\\
\\
and all the promptest of her moon of true tain,\\
though and the song we the brow power,\\
but the moral provice of the crown;\\
the more than with some forms and bright, and power\\
that spote not the spot when that her feet\\
shall to the forth and that power of disming\\
a right thought of countenance and spirit;\\
and smiling and holls the shouch soft the fall and spot\\
of pastage to the strong the noon-to the soun\\
be the halled upon\\


\subsection{Character Level GRU}
The charac

% Linear SVR
\subsection{Word Level GRU}
Memory error due to large size of data size.

\subsection{Word Level LSTM}
\begin{center}
\begin{tabular}{c c}
Epsilon & 0 \\
Penalty parameter C & 1 \\
$\text{R}^{2}$ value & 0.28 \\
\end{tabular}
\end{center}

% Neural Networks
\subsection{Neural Networks}
\begin{center}
\begin{tabular}{c c}
Number of hidden layers & 2 \\
Number of units in layers & 48, 16 \\
Dropout regularization in both layers & 0.1 \\
Activation & ReLu \\
Optimizer & Adam \\
Batch Size & 100 \\
Loss & Mean squared error \\
Epochs & 15 \\ 
$\text{R}^{2}$ value & 0.567 \\
\end{tabular}
\end{center}
%Linear Regression
\subsection{Linear Regression}
Implemented basic linear regression with the Stochastic Gradient Descent algorithm and tuned the\\ \hspace*{4.5mm} learning rate to obtain the highest $\text{R}^{2}$ value possible with no regularization.\\
\hspace*{5mm}$\text{R}^{2}$ value obtained = 0.438

%Random Forest Regression
\subsection{Random Forest Regression}
\begin{center}
\begin{tabular}{c c}
Number of trees in ensemble & 30 \\
Max depth of trees & 8 \\
Cross validation & 5 fold \\
$\text{R}^{2}$ value & 0.489 \\

\end{tabular}
\end{center}

%HMM Regression
\subsection{HMM Regression}
We tried to implement an HMM Regression algorithm to add a generative approach to the regression \hspace*{5mm} problem as well but faced multiple hold ups while calculating the forward and backward probabilities \hspace*{5mm} and hence did not take it to completion.


\section{Comparison}
\subsection*{Quantitative Metrics}
The quantitative metrics used were accuracy, and Lidstone smoothed bigram perplexity.
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 Model & Accuracy & Perplexity \\ [0.5ex] 
 \hline\hline
 Character Level LSTM & 0.5549 & 3.51661 \\ 
 \hline
 Character Level GRU & 0.4852 & 3.72655 \\
 \hline
 Word Level LSTM & 0.9056 & 2.42378 \\
 \hline
 Word Level GRU & 0.9108 & 2.40126 \\ [1ex] 
 \hline
\end{tabular}
\end{center}

It is interesting to note that the character level models appear quantitatively much worse than the word level models despite the poetry samples provided above, and the qualtitative results provided below not quite agreeing with it. 

\subsection*{Qualitative Metric}
I asked three people, each with a fair background in poetry, to rate the poem samples shown above on a integer-valued 100-point scale, with 100 indicating the poem being able to completely pass off as a work by Wordsworth, and 0 indicating that the text is gibberish.

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
 Model & Reviewer 1 & Reviewer 2 & Reviewer 3 \\ [0.5ex] 
 \hline\hline
 Character Level LSTM & 65 & 70 & 70 \\ 
 \hline
 Character Level GRU & 62 & 65 & 68\\
 \hline
 Word Level LSTM & 69 & 75 & 75 \\
 \hline
 Word Level GRU & 71 & 75 & 80 \\ [1ex] 
 \hline
 \end{tabular}
\end{center}
There was a fair consensus amongst the reviewers that the poems would not pass off as Wordsworth because of the lack of meter, questionable grammar, and  the lack of his distinct rhyming schemes, but they were written in a language that would be common in the era, and the poetic themes that he commonly used were used here. Interestingly, the poems generated by both, the character and the word level models were rated close to each other despite the very sharp difference in their quantitative scores.


\section{Conclusion and Future Work}
During our journey of collecting data and understanding and interpreting it, we got in touch with quite \hspace*{5mm} a lot of people who were actively involved in similar fields ranging from Project staff and medical students \hspace*{4.5mm} to professors working on building solutions for problems in related fields (like telepathology) through \hspace*{4.55mm}  whom we learned a lot about the current scenario of such projects. \\\hspace*{4.5mm} These interactions led us to the following conclusions :- \\
\begin{itemize}
\item We could use a similar approach like we did with SEER to obtain data in the Indian context from the Tata Memorial Hospital which would ideally be non-curated and could develop into a valuable project
\item To tackle the problem of the genome sequence data being too large and hence having low workability, we could devise an algorithm which enables us to read and work on sections of the whole data (in the form of sentences instead of whole documents) and then implement our desired approach to identify the probability of cancerous mutations in that section
\item We could expand our context to include images of tissue slices and work to flag abnormalities in these images to make the identification process faster for pathologists who look at enormous amounts of these images
\end{itemize}



\section{References}
\subsection{SEER}
\begin{itemize}
\item \href{https://seer.cancer.gov/seertrack/data/request/}{Request for accessing data}
\item \href{https://seer.cancer.gov/seerstat/software/}{SEER*Stat Installation}
\item \href{https://seer.cancer.gov/seerstat/tutorials/case1a/webprint/}{Tutorial for accessing data}
\end{itemize}
\subsection{The 1000 Genome Project}
\subsection{Others}
\begin{itemize}
\item \href{https://www.ncbi.nlm.nih.gov/pubmed/21763417}{Prediction of cancer causing missense variants}
\item \href{https://www.nature.com/articles/nrclinonc.2013.110.pdf}{Liquid Biopsies for Cancer Genetics}
\item \href{http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=379A5ECA015A0475240A921B9FF1A0AD?doi=10.1.1.78.4479&rep=rep1&type=pdf}{HMM Regression for Life Expectancy Prediction }
\end{itemize}


\end{document}



